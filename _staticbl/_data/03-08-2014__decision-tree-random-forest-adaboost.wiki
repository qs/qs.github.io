title: Decision Tree, Random Forest + AdaBoost
tags: data-mining
date: 03-08-2014 21:22

Нужно описать кратко принципы работы Decision Tree и Random Forest в совокупности с AdaBoost.

====Деревья приятния решений

TL;DR: Строится дерево проверок, в самой вершине - та, что наибольшим образом снижает неопределенность класса, далее так же: вопросы подбираются, чтобы быстрее снизить энтропию.

Типичный вариант задачи: есть множество разных ситуаций (большой набор дискретных/дисретизируемых атрибутов), нужно определить класс ситуации, обычно булевый. 

За уменьшение энтропии отвечает параметр Gain (прирост информации), выбирая следующий узел, нужно взять (по статистике) вопрос с наибольшим Gain. Может быть проблема: прирост информации выбирает атрибуты, для которых больше всего значений. Нужно использовать Gain Ratio, который делит Gain на поправку, зависящую от количества принимаемых значений. 

Оверфитинг - построение слишком подробного дерева, лечится с помощью pruning - обрезания ветвей, со сшиком малым приростом информации.

Достоинства:

* быстрый процесс обучения;
* генерация правил в областях, где эксперту трудно формализовать свои знания;
* извлечение правил на естественном языке;
* интуитивно понятная классификационная модель;
* высокая точность прогноза, сопоставимая с другими методами (статистика, нейронные сети);
* построение непараметрических моделей.


====Случайный лес

TL;DR: Строится куча разных деревьев, обучает деревья на случайно выбранных параметрах, при классификации выбирается средние значения вероятности отнесения к классу.

Пишут, что алгоритм трудно применить неправильно. Out-of-Bag Estimate of Performance - встроенная оценка качества. 

Достоинства:
* нет оверфитинга, если количество атрибутов больше, чем количество объектов для обучения
* можно строить деревья параллельно
* быстро обучается

Недостатки:
* склонен к переобучению на зашумленных данных
* большой размер получающихся моделей


====Adaptive Boosting

TL;DR: на основе множества простых классификаторов делает один мощный

Обучаются слабы классификаторы. Минимизируется взвешенная ошибка классификации, результаты слкдаываются в сильный классификатор.

Достоинства:

* хорошая обобщающая способность
* простота реализации
* идентифицирует объекты, являющиеся шумовыми выбросами

Недостатки:
* переобучение, если много шума
* нужны длинные обучающие выборки
* нужно периодически возвращаться к ранее построенных алгоритмам, чтобы избежать неоптимального набора базовых алгоритмов
* может стать громоздским (использовать много алгоритмов)


====Ссылки:
* [[http://www.amse.ru/archive/courses/2006/nikolenko/notes-01-dectrees.pdf|Деревья принятия решений amse.ru]]
* [[http://www.basegroup.ru/library/analysis/tree/description/|Деревья решений - общие принципы работы basegroup.ru]]
* [[http://www.algorithmist.ru/2012/05/random-forest-r.html|Random Forest с примерами на R]] + комментарии
* [[http://habrahabr.ru/post/80323/|Adaptive boosting // habr]]
* [[http://www.machinelearning.ru/wiki/index.php?title=AdaBoost|Алгоритм AdaBoost // machinelearning.ru]]
* [[http://www.slideshare.net/Technosphere1/l11-35045395|Метод ансамблей // слайды из техносферы mail.ru]]
